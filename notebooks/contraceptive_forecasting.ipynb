{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contraceptive Demand Forecasting \n",
    "\n",
    "Demand forecasting of contraceptive products in Côte D’Ivore.\n",
    "\n",
    "### Dataset: \n",
    "Monthly data of 11 contraceptive products stock across 156 health service delivery sites in the public sector health system in Côte D’Ivore, from January 2016 until June 2019.\n",
    "\n",
    "### Goal:\n",
    "Forecast stock demand for July, August and September of 2019.\n",
    "\n",
    "#### Author: Bruna Correa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates\n",
    "import seaborn as sns\n",
    "\n",
    "# Import forecast libraries\n",
    "from statsmodels.tsa.arima_model import ARIMA,ARIMAResults \n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Neural networks - LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set sns style\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check dataset of contraceptives' stock \n",
    "stock_df = pd.read_csv('/data/Train.csv')\n",
    "print(f'Total number of rows: {stock_df.shape[0]}')\n",
    "print(f'Total number of columns: {stock_df.shape[1]}')\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features information\n",
    "stock_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect features that will be used for EDA and forecasting\n",
    "stock_df_sel = stock_df[['year',\n",
    "                         'month',\n",
    "                         'region',\n",
    "                         'district',\n",
    "                         'site_code',\n",
    "                         'product_code',\n",
    "                         'stock_distributed']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Target variable: 'stock_distributed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "stock_df_sel.isnull().mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize stock_distributed variable\n",
    "stock_df_sel['stock_distributed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by site_code, product_code, year, to check time range completeness:\n",
    "stock_df_sel.groupby(['site_code', 'product_code', 'year']).nunique()['month'].plot(kind='hist')\n",
    "plt.xlabel('Months per year')\n",
    "plt.title('Number of months per year with available data', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are missing months for some combinations of site_code + product_code!\n",
    "* Peak at 6 months: for 2019 we only have data until June.\n",
    "* To do: populate missing months from Jan-2016 until Jun-2019 and impute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of stock_distributed variable\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(a=stock_df_sel['stock_distributed'])\n",
    "plt.title('stock_distributed', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data by region\n",
    "stock_df_sel['region'].value_counts(normalize=True).plot(kind='bar', figsize=(12, 5))\n",
    "plt.title('Stock distributed by region', size=18)\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data by district\n",
    "stock_df_sel['district'].value_counts(normalize=True).plot(kind='bar', figsize=(20, 5))\n",
    "plt.title('Stock distributed by district', size=14)\n",
    "plt.xlabel('District')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore product's distribution\n",
    "stock_df_sel['product_code'].value_counts(normalize=True).plot(kind='bar', figsize=(12, 5))\n",
    "plt.title('Stock distributed by product', size=14)\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of stock_distributed by year\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(y='stock_distributed', \n",
    "            x='year',\n",
    "            data=stock_df_sel,\n",
    "            saturation=1)\n",
    "plt.title('Distribution of stock_distributed by year', fontsize=14)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total stock distributed by year\n",
    "ax = stock_df_sel['year'].value_counts(normalize=False).sort_index().to_frame().transpose().plot.bar()\n",
    "ax.set_title('Total stock_distributed by year', fontsize=14)\n",
    "ax.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distributed stock shows an increase across time from 2016 to 2018\n",
    "* 2019 data is not complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total stock distributed by month\n",
    "ax = stock_df_sel['month'].value_counts(normalize=False).sort_index().to_frame().transpose().plot.bar()\n",
    "ax.set_title('Stock distributed by month',fontsize=14)\n",
    "ax.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the first 6 months, distributed stock shows an increase across time \n",
    "* For the last 6 months, we don't have data from 2019, but the same trend seems to occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2019 data to confirm its month range:\n",
    "stock_df_sel[stock_df_sel['year']==2019]['month'].value_counts(normalize=False).to_frame().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For 2019, there are data until June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datetime formated index for time series analyses\n",
    "idx = pd.to_datetime(\n",
    "    dict(\n",
    "        year=stock_df_sel['year'], \n",
    "        month=stock_df_sel['month'], \n",
    "        day=1 # as we don't have information about the day, fill it with ones\n",
    "    )\n",
    ")\n",
    "# Set dataframe index\n",
    "stock_df_sel.set_index(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stock distributed from 2016 to 2019\n",
    "ax = stock_df_sel.groupby(stock_df_sel.index)['stock_distributed'].sum().plot(figsize=(10,5))\n",
    "ax.set_title('Stock distributed from Jan-2016 to Jun-2019', fontsize=14)\n",
    "ax.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a linear uptrend.\n",
    "* Trend was disrupted in the middle of 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataframe to plot a heatmap\n",
    "monthly = stock_df_sel.resample('M').sum()\n",
    "monthly['month'] = monthly.index.month\n",
    "monthly['year'] = monthly.index.year\n",
    "pv = monthly.pivot(\"month\", \"year\", \"stock_distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stock distributed per year\n",
    "ax = pv.plot(figsize=(10,5))\n",
    "ax.set_title('Stock distributed per year', fontsize=14)\n",
    "ax.set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In 2017 from May to August there was an overall decline in the stock distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the heatmap\n",
    "sns.heatmap(pv, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternative visualization, similar message:\n",
    "    * stock_distribution increased from Jan-2016 until Jun-2019\n",
    "    * there was a decline in 2017 (May - Aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For some combinations of site_code/product, there are missing dates.\n",
    "* Populate missing dates and impute values for stock_distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define a function to populate missing months from Jan-2016 until Jun-2019:\n",
    "\n",
    "def get_ts_range(input_df, date_start, date_end, date_freq):\n",
    "    \n",
    "    # Sort data frame by index:\n",
    "    input_df.sort_index(inplace=True)\n",
    "    \n",
    "    # Create an index with every month in the range from Jan-2016 until Jun-2019:\n",
    "    idx_date = pd.date_range(start=date_start, end=date_end, freq=date_freq)\n",
    "    \n",
    "    # Create a new data frame with complete dates and impute it with zeros:\n",
    "    ts_df = pd.DataFrame(np.zeros(len(idx_date)), index=idx_date)\n",
    "    \n",
    "    # Rename col:\n",
    "    ts_df.columns = ['imputation_col']\n",
    "    \n",
    "    # Concatenate input data frame with complete data frame:\n",
    "    ts_df_comp = pd.concat([ts_df, input_df], axis=1, sort=False)\n",
    "        \n",
    "    # Return data frame with complete range:\n",
    "    return ts_df_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define a function to impute values in created months\n",
    "    \n",
    "def impute_date_vals(input_df, cat_feat_list, target_feat):\n",
    "    \n",
    "    # 1) Impute missing values of stock_distributed with interpolation:\n",
    "    input_df[target_feat].interpolate(method='linear', inplace=True)\n",
    "    \n",
    "    # 2) Impute missing values from begining of time series with back propagation:\n",
    "    input_df[target_feat].fillna(inplace=True, method='bfill')\n",
    "\n",
    "    # 3) Impute missing values from end of time series with forward propagation:\n",
    "    input_df[target_feat].fillna(inplace=True, method='ffill')\n",
    "    \n",
    "    # 4) Impute missing values of categorical features (region, district, site_code, product_code) with mode:\n",
    "    input_df[cat_feat_list] = input_df[cat_feat_list].fillna(input_df.mode().iloc[0])\n",
    "     \n",
    "    # 5) Return complete data frame with desired features\n",
    "    return input_df[cat_feat_list+[target_feat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define function to fill and impute individual time series (site_code & product_code)\n",
    "\n",
    "def get_complete_df(input_df, date_start, date_end, date_freq, cat_feat_list, target_feat):\n",
    "    \n",
    "    # Create output place holder df\n",
    "    df_complete = pd.DataFrame()\n",
    "    \n",
    "    # Loop over data frame and get every 'site_code' & 'product_code' combinations:\n",
    "    for site, product in input_df.groupby(['site_code','product_code']).indices:\n",
    "        \n",
    "        # Create a dataframe for each time series:\n",
    "        single_ts_df = input_df[(input_df['site_code'] == site) & (input_df['product_code'] == product)]\n",
    "        \n",
    "        # Add missing months:\n",
    "        single_ts_df_complete_dates = get_ts_range(single_ts_df, \n",
    "                                                   date_start, \n",
    "                                                   date_end, \n",
    "                                                   date_freq)\n",
    "        \n",
    "        # Impute missing values:\n",
    "        single_ts_df_complete_dates_vals = impute_date_vals(single_ts_df_complete_dates, \n",
    "                                                            cat_feat_list,\n",
    "                                                            target_feat)\n",
    "        \n",
    "        # Concatenate individual data frames togetherL\n",
    "        df_complete = pd.concat([df_complete, single_ts_df_complete_dates_vals[cat_feat_list+[target_feat]]])\n",
    "    \n",
    "    # Return complete data frame\n",
    "    return df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run functions to get a populated and imputed data frame:\n",
    "stock_df_sel_comp = get_complete_df(input_df = stock_df_sel, \n",
    "                                    date_start = '2016-01-01', \n",
    "                                    date_end = '2019-06-01', \n",
    "                                    date_freq = 'MS',\n",
    "                                    cat_feat_list = ['region', 'district', 'site_code', 'product_code'],\n",
    "                                    target_feat = 'stock_distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df info:\n",
    "stock_df_sel_comp.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by index to check completeness:\n",
    "stock_df_sel_comp.groupby(stock_df_sel_comp.index).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values:\n",
    "stock_df_sel_comp.isnull().mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA - Models - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data frame formating:\n",
    "stock_df_sel_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run ARIMA with auto_arima for order selection:\n",
    "\n",
    "def run_auto_arima(input_df, target_feat, train_start, train_end, pred_months):\n",
    "   \n",
    "    # Create a placeholder to save prediction losses:\n",
    "    error = []\n",
    "\n",
    "    # Loop over data frame to get predictions for each individual time series:\n",
    "    for site, product in input_df.groupby(['site_code','product_code']).indices:\n",
    "\n",
    "        # Create a dataframe for each combination of site_code + product code:\n",
    "        single_ts = input_df[(input_df['site_code'] == site) & (input_df['product_code'] == product)]\n",
    "\n",
    "        # Set index to monthly data:\n",
    "        single_ts.index.freq = 'MS'\n",
    "\n",
    "        # Split data in train and test (last 3 months):\n",
    "        train = single_ts.loc[train_start:train_end][target_feat]\n",
    "        test = single_ts.loc[train_end:][target_feat]\n",
    "\n",
    "        # Run auto_arima to determine ARIMA orders (p,d,q):\n",
    "        arima_model = auto_arima(train, \n",
    "                                 start_p=0, \n",
    "                                 start_d=0,\n",
    "                                 start_q=0,\n",
    "                                 max_p=6,\n",
    "                                 max_d=6,\n",
    "                                 max_q=6,\n",
    "                                 seasonal=False,\n",
    "                                 random_state=42)\n",
    "        #arima_model.summary()\n",
    "\n",
    "        # Use model to generate predictions for a given number of months:\n",
    "        prediction = arima_model.predict(n_periods=pred_months)\n",
    "\n",
    "        # Create predictions data frame to plot:\n",
    "        #prediction_df = pd.DataFrame(prediction, index=test.index)\n",
    "        #oprediction_df.columns = [str('predicted_'+ target_feat)]\n",
    "\n",
    "        # Plot: train, test and predicted values:\n",
    "        #plt.figure(figsize=(8,5))\n",
    "        #plt.plot(train)\n",
    "        #plt.plot(test)\n",
    "        #plt.plot(prediction_df)\n",
    "        #plt.xlabel('Date')\n",
    "        #plt.ylabel('stock_distributed')\n",
    "        #plt.title(site + '_' + product + ': stock distributed across time')\n",
    "        #plt.legend(['Train', 'Test', 'Prediction'])\n",
    "        #plt.savefig('/plots/plot_'+ site + '_' + product + '.pdf')\n",
    "\n",
    "        # Compare predictions with test, compute RMSE and append to losses:\n",
    "        error.append(rmse(test, prediction))\n",
    "    \n",
    "    # Return average RMSE for all model predictions:\n",
    "    return np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ARIMA and get average loss for all time series predictions:\n",
    "avg_rmse_arima = run_auto_arima(input_df = stock_df_sel_comp, \n",
    "                                target_feat = 'stock_distributed',\n",
    "                                train_start = '2016-01-01',\n",
    "                                train_end = '2019-04-01',\n",
    "                                pred_months = 3) # predictions for 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store losses:\n",
    "loss_dict = {'rmse_arima': avg_rmse_arima}\n",
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {'rmse_arima': 9.06582051174892}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since trend was interrupted in 2017, try to do predictions starting from 2017-August\n",
    "avg_rmse_arima_2017 = run_auto_arima(input_df = stock_df_sel_comp, \n",
    "                                     target_feat = 'stock_distributed',\n",
    "                                     train_start = '2017-08-01', # change start month for Aug-2017\n",
    "                                     train_end = '2019-04-01',\n",
    "                                     pred_months = 3) # predictions for 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to losses dict:\n",
    "loss_dict['rmse_arima_2017'] = avg_rmse_arima_2017\n",
    "pd.DataFrame(loss_dict, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subdata frame with one time series\n",
    "# Subselect one combination of district and product code to test; then loop over all combinations\n",
    "site = 'C1086'\n",
    "product = 'AS27133'\n",
    "\n",
    "#for site, product in stock_df_sel_comp.groupby(['site_code','product_code']).indices:\n",
    "\n",
    "# Create a dataframe for each combination of site_code + product code\n",
    "single_ts = stock_df_sel_comp[(stock_df_sel_comp['site_code'] == site) & (stock_df_sel_comp['product_code'] == product)]\n",
    "\n",
    "# Set index to monthly data\n",
    "single_ts.index.freq = 'MS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "\n",
    "train = single_ts[['stock_distributed']].iloc[:39]\n",
    "test = single_ts[['stock_distributed']].iloc[39:]\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on the train data only, otherwise we using information from the test set\n",
    "scaler.fit(train)\n",
    "\n",
    "# Scale train and test\n",
    "scaled_train = scaler.transform(train)\n",
    "scaled_test = scaler.transform(test)\n",
    "\n",
    "# Time series generator\n",
    "# Define time series generator\n",
    "# Let's redefine to get 12 months back and then predict the next month out\n",
    "n_input = 38\n",
    "n_features = 1\n",
    "generator = TimeseriesGenerator(scaled_train, \n",
    "                                scaled_train, \n",
    "                                length=n_input, \n",
    "                                batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, activation='relu', input_shape=(n_input, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit_generator(generator, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per epoch\n",
    "loss_per_epoch = model.history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "test_predictions = []\n",
    "\n",
    "first_eval_batch = scaled_train[-n_input:]\n",
    "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # Get prediction for 1 month ahead:\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "    # Store prediction:\n",
    "    test_predictions.append(current_pred) \n",
    "    \n",
    "    # Update batch: include prediction and drop first value:\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale to original scale\n",
    "true_predictions = scaler.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predictions'] = true_predictions\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RMSE\n",
    "rmse(test['stock_distributed'], test['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "test.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "my_model = load_model('rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to scale data\n",
    "\n",
    "def data_scaler(input_train, input_test):\n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit on the train data only, otherwise we using information from the test set\n",
    "    scaler.fit(input_train)\n",
    "\n",
    "    # Scale train and test\n",
    "    scaled_train = scaler.transform(input_train)\n",
    "    scaled_test = scaler.transform(input_test)\n",
    "    \n",
    "    return scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa = stock_df_sel_comp[stock_df_sel_comp['site_code'] == 'C1066']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model:\n",
    "\n",
    "n_input = 38\n",
    "n_features = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, activation='relu', input_shape=(n_input, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define function to build and get predictions from recurrent neural network with LSTM:\n",
    "\n",
    "def my_lstm_model(input_df, input_model, n_input, n_features, target_feat, n_epochs):\n",
    "\n",
    "    # Create a placeholder to save prediction losses:\n",
    "    error = []\n",
    "\n",
    "    # Loop over data frame to get predictions for each individual time series:\n",
    "    for site, product in input_df.groupby(['site_code','product_code']).indices:\n",
    "    \n",
    "        # Create a data frame per each time series:\n",
    "        single_ts = input_df[(input_df['site_code'] == site) & (input_df['product_code'] == product)]\n",
    "\n",
    "        # Set index to monthly data:\n",
    "        single_ts.index.freq = 'MS'\n",
    "    \n",
    "        # Train / test split:\n",
    "        train = single_ts[[target_feat]].iloc[:39]\n",
    "        test = single_ts[[target_feat]].iloc[39:]\n",
    "    \n",
    "        # Scale data:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # Fit on the train data only, otherwise we using information from the test set\n",
    "        scaler.fit(train)\n",
    "\n",
    "        # Scale train and test:\n",
    "        scaled_train = scaler.transform(train)\n",
    "        scaled_test = scaler.transform(test)\n",
    "    \n",
    "        # Define time series generator:\n",
    "        generator = TimeseriesGenerator(scaled_train, \n",
    "                                        scaled_train, \n",
    "                                        length=n_input, \n",
    "                                        batch_size=1)\n",
    "\n",
    "        # Fit model:\n",
    "        input_model.fit_generator(generator, \n",
    "                                  epochs=n_epochs,\n",
    "                                  verbose=0)\n",
    "\n",
    "        # Plot loss per epoch\n",
    "        #loss_per_epoch = model.history.history['loss']\n",
    "        #plt.plot(range(len(loss_per_epoch)), loss_per_epoch)\n",
    "\n",
    "        # Test predictions:\n",
    "        test_predictions = []\n",
    "\n",
    "        first_eval_batch = scaled_train[-n_input:]\n",
    "        current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "        # Loop to get predictions:\n",
    "        for i in range(len(test)):\n",
    "    \n",
    "            # Get prediction for 1 month ahead:\n",
    "            current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "            # Store prediction:\n",
    "            test_predictions.append(current_pred) \n",
    "    \n",
    "            # Update batch: include prediction and drop first value:\n",
    "            current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "        \n",
    "        # Re-scale predictions to match original scale:\n",
    "        true_predictions = scaler.inverse_transform(test_predictions)\n",
    "    \n",
    "        # Add predictions to test df to calculate loss:\n",
    "        test['predictions'] = true_predictions\n",
    "    \n",
    "        # Plot test vs. predictions:\n",
    "        #test.plot(figsize=(5,5))\n",
    "    \n",
    "        # Compute RMSE:\n",
    "        error.append(rmse(test[target_feat], test['predictions']))\n",
    "        \n",
    "        # Save model:\n",
    "        #model.save(product + '_' + site + '_rnn_model.h5')\n",
    "    \n",
    "    # Return average RMSE for all model predictions:\n",
    "    return np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run lstm model\n",
    "avg_rmse_lstm = my_lstm_model(input_df = stock_df_sel_comp, \n",
    "                              input_model = model,\n",
    "                              n_input = n_input, \n",
    "                              n_features = n_features, \n",
    "                              target_feat = 'stock_distributed',\n",
    "                              n_epochs = 20)\n",
    "\n",
    "# Add to losses dict:\n",
    "loss_dict['rmse_rnn_lstm'] = avg_rmse_lstm\n",
    "pd.DataFrame(loss_dict, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Prophet\n",
    "#! conda create --name myenv --yes\n",
    "#! conda activate myenv --yes\n",
    "#! conda install -c conda-forge fbprophet --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Prophet\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Format data\n",
    "df.columns = ['ds','y']\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# Train / test split\n",
    "train = df.iloc[:576]\n",
    "test = df.iloc[576:]\n",
    "\n",
    "# Create and fit the model \n",
    "m = Prophet()\n",
    "m.fit(train)\n",
    "\n",
    "# Create a future placeholder\n",
    "future = m.make_future_dataframe(periods=3,freq='MS')\n",
    "\n",
    "# Predict and fill future\n",
    "forecast = m.predict(future)\n",
    "\n",
    "# Plot\n",
    "ax = forecast.plot(x='ds',y='yhat',label='Predictions',legend=True,figsize=(12,8))\n",
    "test.plot(x='ds',y='y',label='True Miles',legend=True,ax=ax,xlim=('2019-04-01','2019-06-01'))\n",
    "\n",
    "# Compute rmse\n",
    "predictions = forecast.iloc[-3:]['yhat']\n",
    "rmse(predictions,test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
